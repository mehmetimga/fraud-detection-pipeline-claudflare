---
title: "Fraud Detection Model Analysis"
subtitle: "Complete Pipeline: Preprocess â†’ Train â†’ Evaluate"
author: "ML Pipeline Analysis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    theme: flatly
    highlight: tango
    code_folding: show
    df_print: paged
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = "center",
  cache = FALSE
)

# Set seed for reproducibility
set.seed(42)
```

# Overview

This notebook implements the complete fraud detection pipeline matching the **Rust project** and **Python scripts**:

1. **Preprocess**: Load data, engineer features (matching `preprocess.py`)
2. **Train**: Train 4 models - XGBoost, CatBoost, LightGBM, Random Forest (matching `train.py`)
3. **Evaluate**: Compare models, create ensemble, analyze XGBoost vs CatBoost (matching `evaluate.py`)

## Load Libraries

```{r libraries}
# Core
library(tidyverse)
library(caret)

# Models (same as Rust project)
library(xgboost)
library(lightgbm)
library(catboost)
library(randomForest)

# Evaluation
library(pROC)
library(MLmetrics)

# Visualization
library(knitr)
library(kableExtra)
library(scales)

cat("âœ“ All libraries loaded\n")
```

# Preprocessing {.tabset}

This section matches `scripts/python/src/preprocess.py`.

## Load Data

```{r load-data}
# Load UCI Credit Card dataset
data_path <- "data/UCI_Credit_Card.csv"

if (!file.exists(data_path)) {
  stop("Data file not found! Place UCI_Credit_Card.csv in data/ folder")
}

df <- read_csv(data_path, show_col_types = FALSE)

# Rename columns to match expected format
df <- df %>%
  select(-ID) %>%
  rename(
    default = `default.payment.next.month`,
    PAY_1 = PAY_0  # Rename for consistency
  )

# Clean categorical values
df <- df %>%
  mutate(
    EDUCATION = ifelse(EDUCATION %in% c(0, 5, 6), 4, EDUCATION),
    MARRIAGE = ifelse(MARRIAGE == 0, 3, MARRIAGE)
  )

cat(sprintf("Loaded %d records with %d columns\n", nrow(df), ncol(df)))
cat(sprintf("Default rate: %.1f%%\n", mean(df$default) * 100))
```

## Feature Engineering

```{r feature-engineering}
# Feature engineering matching Rust project and preprocess.py

engineer_features <- function(data) {
  data <- data %>%
    mutate(
      # --- Payment Behavior Features ---
      # Average payment delay
      avg_pay_delay = rowMeans(select(., PAY_1:PAY_6)),
      
      # Maximum payment delay
      max_pay_delay = pmax(PAY_1, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6),
      
      # Number of months with delay (payment > 0)
      months_delayed = (PAY_1 > 0) + (PAY_2 > 0) + (PAY_3 > 0) + 
                       (PAY_4 > 0) + (PAY_5 > 0) + (PAY_6 > 0),
      
      # Payment trend (improving or worsening)
      pay_trend = PAY_1 - PAY_6,
      
      # --- Bill Amount Features ---
      # Average bill amount
      avg_bill_amt = rowMeans(select(., BILL_AMT1:BILL_AMT6)),
      
      # Bill trend
      bill_trend = BILL_AMT1 - BILL_AMT6,
      
      # Bill volatility (standard deviation)
      bill_volatility = apply(select(., BILL_AMT1:BILL_AMT6), 1, sd),
      
      # --- Payment Amount Features ---
      # Average payment amount
      avg_pay_amt = rowMeans(select(., PAY_AMT1:PAY_AMT6)),
      
      # Total payment amount
      total_pay_amt = PAY_AMT1 + PAY_AMT2 + PAY_AMT3 + PAY_AMT4 + PAY_AMT5 + PAY_AMT6,
      
      # --- Utilization Features ---
      # Credit utilization ratio
      utilization_ratio = BILL_AMT1 / (LIMIT_BAL + 1),
      
      # Over limit indicator
      over_limit = as.integer(BILL_AMT1 > LIMIT_BAL),
      
      # Payment to bill ratio
      pay_bill_ratio = PAY_AMT1 / (abs(BILL_AMT1) + 1),
      
      # --- Demographic Features ---
      # Age groups (0-4)
      age_group = case_when(
        AGE <= 25 ~ 0,
        AGE <= 35 ~ 1,
        AGE <= 45 ~ 2,
        AGE <= 55 ~ 3,
        TRUE ~ 4
      )
    )
  
  return(data)
}

# Apply feature engineering
df <- engineer_features(df)

cat(sprintf("Features after engineering: %d\n", ncol(df) - 1))  # -1 for target

# Show new features
new_features <- c("avg_pay_delay", "max_pay_delay", "months_delayed", "pay_trend",
                  "avg_bill_amt", "bill_trend", "bill_volatility",
                  "avg_pay_amt", "total_pay_amt",
                  "utilization_ratio", "over_limit", "pay_bill_ratio", "age_group")

cat("\nEngineered features:\n")
cat(paste("  -", new_features, collapse = "\n"))
```

## Train/Test Split

```{r train-test-split}
# Split data (80/20) with stratification
train_idx <- createDataPartition(df$default, p = 0.8, list = FALSE)

train_data <- df[train_idx, ]
test_data <- df[-train_idx, ]

# Separate features and target
X_train <- train_data %>% select(-default)
y_train <- train_data$default

X_test <- test_data %>% select(-default)
y_test <- test_data$default

# Summary
cat(sprintf("\nTrain: %d samples (%.1f%% default)\n", nrow(X_train), mean(y_train) * 100))
cat(sprintf("Test: %d samples (%.1f%% default)\n", nrow(X_test), mean(y_test) * 100))
cat(sprintf("Features: %d\n", ncol(X_train)))

# Create validation set from training data
val_idx <- createDataPartition(y_train, p = 0.1, list = FALSE)
X_val <- X_train[val_idx, ]
y_val <- y_train[val_idx]
X_train_final <- X_train[-val_idx, ]
y_train_final <- y_train[-val_idx]

cat(sprintf("\nFinal splits:\n"))
cat(sprintf("  Training: %d samples\n", nrow(X_train_final)))
cat(sprintf("  Validation: %d samples\n", nrow(X_val)))
cat(sprintf("  Test: %d samples\n", nrow(X_test)))
```

<br>

# Model Training {.tabset}

This section matches `scripts/python/src/train.py`. We train all 4 models used in the Rust project.

## XGBoost

```{r train-xgboost}
cat("--- Training XGBoost ---\n")

# Calculate scale_pos_weight for imbalanced data
scale_pos_weight <- sum(y_train_final == 0) / sum(y_train_final == 1)
cat(sprintf("Scale pos weight: %.2f\n", scale_pos_weight))

# Prepare DMatrix
dtrain <- xgb.DMatrix(
  data = as.matrix(X_train_final),
  label = y_train_final
)
dval <- xgb.DMatrix(
  data = as.matrix(X_val),
  label = y_val
)

# Train with early stopping
xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  max_depth = 6,
  eta = 0.05,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8,
  scale_pos_weight = scale_pos_weight
)

xgb_model <- xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = 500,
  watchlist = list(train = dtrain, val = dval),
  early_stopping_rounds = 50,
  verbose = 0
)

cat(sprintf("âœ“ XGBoost trained (%d rounds)\n", xgb_model$best_iteration))
```

## LightGBM

```{r train-lightgbm}
cat("--- Training LightGBM ---\n")

# Prepare datasets
lgb_train <- lgb.Dataset(
  data = as.matrix(X_train_final),
  label = y_train_final
)
lgb_val <- lgb.Dataset(
  data = as.matrix(X_val),
  label = y_val,
  reference = lgb_train
)

# Train with early stopping
lgb_params <- list(
  objective = "binary",
  metric = "auc",
  learning_rate = 0.05,
  max_depth = 6,
  num_leaves = 31,
  min_data_in_leaf = 20,
  bagging_fraction = 0.8,
  feature_fraction = 0.8,
  is_unbalance = TRUE,
  verbose = -1
)

lgb_model <- lgb.train(
  params = lgb_params,
  data = lgb_train,
  nrounds = 500,
  valids = list(val = lgb_val),
  early_stopping_rounds = 50,
  verbose = -1
)

cat(sprintf("âœ“ LightGBM trained (%d rounds)\n", lgb_model$best_iter))
```

## CatBoost

```{r train-catboost}
cat("--- Training CatBoost ---\n")

# Prepare pools
cb_train <- catboost.load_pool(
  data = as.matrix(X_train_final),
  label = y_train_final
)
cb_val <- catboost.load_pool(
  data = as.matrix(X_val),
  label = y_val
)

# Train with early stopping
cb_params <- list(
  loss_function = "Logloss",
  eval_metric = "AUC",
  iterations = 500,
  learning_rate = 0.05,
  depth = 6,
  l2_leaf_reg = 3,
  auto_class_weights = "Balanced",
  early_stopping_rounds = 50,
  verbose = 50
)

cb_model <- catboost.train(
  learn_pool = cb_train,
  test_pool = cb_val,
  params = cb_params
)

cat(sprintf("âœ“ CatBoost trained\n"))
```

## Random Forest

```{r train-random-forest}
cat("--- Training Random Forest ---\n")

# Train Random Forest
rf_model <- randomForest(
  x = X_train_final,
  y = as.factor(y_train_final),
  ntree = 200,
  maxnodes = 100,
  classwt = c("0" = 1, "1" = scale_pos_weight),
  importance = TRUE
)

cat(sprintf("âœ“ Random Forest trained (%d trees)\n", rf_model$ntree))
```

## Models Summary

```{r models-summary}
models <- list(
  xgboost = xgb_model,
  lightgbm = lgb_model,
  catboost = cb_model,
  random_forest = rf_model
)

cat("\n=== Models Trained ===\n")
cat(sprintf("  1. XGBoost: %d iterations\n", xgb_model$best_iteration))
cat(sprintf("  2. LightGBM: %d iterations\n", lgb_model$best_iter))
cat(sprintf("  3. CatBoost: trained with early stopping\n"))
cat(sprintf("  4. Random Forest: %d trees\n", rf_model$ntree))
```

<br>

# Model Evaluation {.tabset}

This section matches `scripts/python/src/evaluate.py`.

## Get Predictions

```{r get-predictions}
# Optimal thresholds from analysis (matching Rust project)
OPTIMAL_THRESHOLDS <- list(
  xgboost = 0.61,
  catboost = 0.57,
  lightgbm = 0.60,
  random_forest = 0.33,
  ensemble = 0.56
)

# Model weights for ensemble (matching Rust project)
MODEL_WEIGHTS <- list(
  xgboost = 0.30,
  catboost = 0.28,
  lightgbm = 0.25,
  random_forest = 0.17
)

# Get predictions from each model
get_predictions <- function() {
  preds <- list()
  
  # XGBoost
  dtest <- xgb.DMatrix(data = as.matrix(X_test))
  preds$xgboost <- predict(xgb_model, dtest)
  
  # LightGBM
  preds$lightgbm <- predict(lgb_model, as.matrix(X_test))
  
  # CatBoost
  cb_test <- catboost.load_pool(data = as.matrix(X_test))
  preds$catboost <- catboost.predict(cb_model, cb_test, prediction_type = "Probability")
  
  # Random Forest
  preds$random_forest <- predict(rf_model, X_test, type = "prob")[, 2]
  
  return(preds)
}

predictions <- get_predictions()

cat("âœ“ Predictions generated for all models\n")
```

## Evaluate Each Model

```{r evaluate-models}
# Function to find optimal threshold
find_optimal_threshold <- function(y_true, y_proba) {
  thresholds <- seq(0.1, 0.9, by = 0.01)
  f1_scores <- sapply(thresholds, function(t) {
    y_pred <- as.integer(y_proba >= t)
    if (sum(y_pred) == 0) return(0)
    F1_Score(y_true, y_pred)
  })
  best_idx <- which.max(f1_scores)
  return(list(threshold = thresholds[best_idx], f1 = f1_scores[best_idx]))
}

# Function to evaluate a model
evaluate_model <- function(y_true, y_proba, threshold, model_name) {
  y_pred <- as.integer(y_proba >= threshold)
  
  metrics <- list(
    model = model_name,
    accuracy = Accuracy(y_pred, y_true),
    precision = Precision(y_true, y_pred),
    recall = Recall(y_true, y_pred),
    f1 = F1_Score(y_true, y_pred),
    roc_auc = as.numeric(auc(roc(y_true, y_proba, quiet = TRUE))),
    threshold_used = threshold
  )
  
  # Find optimal threshold
  opt <- find_optimal_threshold(y_true, y_proba)
  metrics$optimal_threshold <- opt$threshold
  
  return(metrics)
}

# Evaluate all models
results <- list()
for (name in names(predictions)) {
  threshold <- OPTIMAL_THRESHOLDS[[name]]
  results[[name]] <- evaluate_model(y_test, predictions[[name]], threshold, name)
}

# Convert to data frame
results_df <- bind_rows(results)

cat("=== Individual Model Results ===\n\n")
for (name in names(predictions)) {
  r <- results[[name]]
  cat(sprintf("--- %s ---\n", toupper(name)))
  cat(sprintf("  Threshold used: %.2f\n", r$threshold_used))
  cat(sprintf("  Optimal threshold: %.2f\n", r$optimal_threshold))
  cat(sprintf("  Accuracy: %.4f\n", r$accuracy))
  cat(sprintf("  Precision: %.4f\n", r$precision))
  cat(sprintf("  Recall: %.4f\n", r$recall))
  cat(sprintf("  F1 Score: %.4f\n", r$f1))
  cat(sprintf("  ROC-AUC: %.4f\n\n", r$roc_auc))
}
```

## Ensemble Evaluation

```{r ensemble-evaluation}
cat("=== Ensemble Evaluation ===\n\n")

# Create weighted ensemble predictions
ensemble_predict <- function(predictions, weights = MODEL_WEIGHTS) {
  weighted_sum <- rep(0, length(predictions[[1]]))
  total_weight <- 0
  
  for (name in names(predictions)) {
    if (name %in% names(weights)) {
      weighted_sum <- weighted_sum + predictions[[name]] * weights[[name]]
      total_weight <- total_weight + weights[[name]]
    }
  }
  
  return(weighted_sum / total_weight)
}

# Get ensemble predictions
ensemble_proba <- ensemble_predict(predictions)

# Evaluate ensemble
ensemble_threshold <- OPTIMAL_THRESHOLDS$ensemble
ensemble_results <- evaluate_model(y_test, ensemble_proba, ensemble_threshold, "ensemble")

# Add to results
results_df <- bind_rows(results_df, ensemble_results)

cat(sprintf("Weights: XGBoost=%.0f%%, CatBoost=%.0f%%, LightGBM=%.0f%%, RF=%.0f%%\n",
            MODEL_WEIGHTS$xgboost * 100, MODEL_WEIGHTS$catboost * 100,
            MODEL_WEIGHTS$lightgbm * 100, MODEL_WEIGHTS$random_forest * 100))
cat(sprintf("Threshold: %.2f\n", ensemble_threshold))
cat(sprintf("Accuracy: %.4f\n", ensemble_results$accuracy))
cat(sprintf("Precision: %.4f\n", ensemble_results$precision))
cat(sprintf("Recall: %.4f\n", ensemble_results$recall))
cat(sprintf("F1 Score: %.4f\n", ensemble_results$f1))
cat(sprintf("ROC-AUC: %.4f\n", ensemble_results$roc_auc))
```

## Results Table

```{r results-table}
# Sort by ROC-AUC
results_df <- results_df %>% arrange(desc(roc_auc))

results_df %>%
  mutate(
    model = toupper(model),
    accuracy = sprintf("%.4f", accuracy),
    precision = sprintf("%.4f", precision),
    recall = sprintf("%.4f", recall),
    f1 = sprintf("%.4f", f1),
    roc_auc = sprintf("%.4f", roc_auc),
    threshold_used = sprintf("%.2f", threshold_used)
  ) %>%
  select(Model = model, `ROC-AUC` = roc_auc, F1 = f1, 
         Precision = precision, Recall = recall, 
         Accuracy = accuracy, Threshold = threshold_used) %>%
  kable(align = "lrrrrrr") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  row_spec(which(results_df$model == "ensemble"), bold = TRUE, background = "#e8f4f8") %>%
  row_spec(1, bold = TRUE, color = "#27ae60")
```

<br>

# XGBoost vs CatBoost {.tabset}

Detailed comparison of the two top gradient boosting models.

## Metric Comparison

```{r xgb-cb-comparison}
xgb <- results_df %>% filter(model == "xgboost")
cb <- results_df %>% filter(model == "catboost")

comparison <- tibble(
  Metric = c("ROC-AUC", "F1 Score", "Precision", "Recall", "Accuracy"),
  XGBoost = c(xgb$roc_auc, xgb$f1, xgb$precision, xgb$recall, xgb$accuracy),
  CatBoost = c(cb$roc_auc, cb$f1, cb$precision, cb$recall, cb$accuracy)
) %>%
  mutate(
    Difference = XGBoost - CatBoost,
    Winner = case_when(
      Difference > 0.001 ~ "XGBoost",
      Difference < -0.001 ~ "CatBoost",
      TRUE ~ "Tie"
    )
  )

comparison %>%
  mutate(
    XGBoost = sprintf("%.4f", XGBoost),
    CatBoost = sprintf("%.4f", CatBoost),
    Difference = sprintf("%+.4f", as.numeric(Difference))
  ) %>%
  kable(align = "lrrrr") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(5, color = ifelse(comparison$Winner == "XGBoost", "#27ae60", 
                                ifelse(comparison$Winner == "CatBoost", "#e74c3c", "gray")))
```

## Visual Comparison

```{r xgb-cb-chart, fig.height=5}
xgb_cb_data <- results_df %>%
  filter(model %in% c("xgboost", "catboost")) %>%
  select(model, roc_auc, f1, precision, recall, accuracy) %>%
  pivot_longer(-model, names_to = "metric", values_to = "value")

ggplot(xgb_cb_data, aes(x = metric, y = value, fill = model)) +
  geom_col(position = "dodge", width = 0.7) +
  geom_text(aes(label = sprintf("%.3f", value)), 
            position = position_dodge(width = 0.7), vjust = -0.5, size = 3.5) +
  scale_fill_manual(values = c("catboost" = "#e74c3c", "xgboost" = "#3498db"),
                    labels = c("CatBoost", "XGBoost")) +
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.15))) +
  labs(
    title = "XGBoost vs CatBoost: Head-to-Head Comparison",
    subtitle = "Comparing key performance metrics",
    x = NULL, y = "Score", fill = "Model"
  ) +
  theme_minimal() +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

## ROC Curves

```{r roc-curves, fig.height=5}
# Create ROC curves
roc_xgb <- roc(y_test, predictions$xgboost, quiet = TRUE)
roc_cb <- roc(y_test, predictions$catboost, quiet = TRUE)
roc_lgb <- roc(y_test, predictions$lightgbm, quiet = TRUE)
roc_rf <- roc(y_test, predictions$random_forest, quiet = TRUE)
roc_ens <- roc(y_test, ensemble_proba, quiet = TRUE)

# Plot
plot(roc_xgb, col = "#3498db", lwd = 2, main = "ROC Curves - All Models")
lines(roc_cb, col = "#e74c3c", lwd = 2)
lines(roc_lgb, col = "#2ecc71", lwd = 2)
lines(roc_rf, col = "#9b59b6", lwd = 2)
lines(roc_ens, col = "#f39c12", lwd = 3, lty = 2)

legend("bottomright", 
       legend = c(
         sprintf("XGBoost (AUC: %.3f)", auc(roc_xgb)),
         sprintf("CatBoost (AUC: %.3f)", auc(roc_cb)),
         sprintf("LightGBM (AUC: %.3f)", auc(roc_lgb)),
         sprintf("Random Forest (AUC: %.3f)", auc(roc_rf)),
         sprintf("Ensemble (AUC: %.3f)", auc(roc_ens))
       ),
       col = c("#3498db", "#e74c3c", "#2ecc71", "#9b59b6", "#f39c12"),
       lwd = c(2, 2, 2, 2, 3),
       lty = c(1, 1, 1, 1, 2))
```

## Summary

```{r xgb-cb-summary}
xgb_wins <- sum(comparison$Winner == "XGBoost")
cb_wins <- sum(comparison$Winner == "CatBoost")
ties <- sum(comparison$Winner == "Tie")
winner <- ifelse(xgb_wins > cb_wins, "XGBoost", 
                 ifelse(cb_wins > xgb_wins, "CatBoost", "Tie"))

cat("=== XGBoost vs CatBoost Summary ===\n\n")
cat(sprintf("Overall Winner: %s\n", winner))
cat(sprintf("XGBoost Wins: %d metrics\n", xgb_wins))
cat(sprintf("CatBoost Wins: %d metrics\n", cb_wins))
cat(sprintf("Ties: %d metrics\n", ties))
cat(sprintf("\nROC-AUC Difference: %+.4f\n", xgb$roc_auc - cb$roc_auc))
```

<br>

# Ensemble Analysis {.tabset}

## Configuration

```{r ensemble-config}
weights_df <- tibble(
  Model = c("XGBoost", "CatBoost", "LightGBM", "Random Forest"),
  Weight = c(0.30, 0.28, 0.25, 0.17),
  Percentage = paste0(c(30, 28, 25, 17), "%"),
  Reason = c("Best ROC-AUC", "Second best", "Third", "Fourth (baseline)")
)

weights_df %>%
  kable(align = "lrlr") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Ensemble vs Best Model

```{r ensemble-vs-best, fig.height=5}
best_single <- results_df %>% 
  filter(model != "ensemble") %>%
  arrange(desc(roc_auc)) %>%
  slice(1)

ensemble <- results_df %>% filter(model == "ensemble")

comp_data <- tibble(
  Model = c(toupper(best_single$model), "ENSEMBLE"),
  `ROC-AUC` = c(best_single$roc_auc, ensemble$roc_auc),
  F1 = c(best_single$f1, ensemble$f1),
  Precision = c(best_single$precision, ensemble$precision),
  Recall = c(best_single$recall, ensemble$recall)
) %>%
  pivot_longer(-Model, names_to = "Metric", values_to = "Value")

ggplot(comp_data, aes(x = Metric, y = Value, fill = Model)) +
  geom_col(position = "dodge", width = 0.6) +
  geom_text(aes(label = sprintf("%.3f", Value)), 
            position = position_dodge(width = 0.6), vjust = -0.5, size = 4) +
  scale_fill_manual(values = c("#2ecc71", "#9b59b6")) +
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.15))) +
  labs(
    title = sprintf("Ensemble vs %s (Best Single Model)", toupper(best_single$model)),
    x = NULL, y = "Score"
  ) +
  theme_minimal() +
  theme(legend.position = "top")
```

## Benefits

```{r ensemble-benefits}
roc_improvement <- (ensemble$roc_auc - best_single$roc_auc) / best_single$roc_auc * 100

benefits <- tibble(
  Benefit = c("ROC-AUC Change", "Robustness", "Variance Reduction", "Best Use Case"),
  Description = c(
    sprintf("%+.2f%% vs best single model", roc_improvement),
    "Less sensitive to individual model failures",
    "Averaging reduces prediction variance",
    "High-stakes decisions requiring maximum accuracy"
  )
)

benefits %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

<br>

# Model Rankings

```{r rankings, fig.height=5}
# Create ranking
rankings <- results_df %>%
  filter(model != "ensemble") %>%
  mutate(
    rank_auc = rank(-roc_auc),
    rank_f1 = rank(-f1),
    rank_precision = rank(-precision),
    rank_recall = rank(-recall),
    avg_rank = (rank_auc + rank_f1 + rank_precision + rank_recall) / 4
  ) %>%
  arrange(avg_rank) %>%
  mutate(overall_rank = row_number())

cat("=== Model Rankings (by ROC-AUC) ===\n\n")
for (i in 1:nrow(results_df)) {
  r <- results_df[i, ]
  emoji <- ifelse(i == 1, "ðŸ¥‡", ifelse(i == 2, "ðŸ¥ˆ", ifelse(i == 3, "ðŸ¥‰", "  ")))
  cat(sprintf("%s #%d %s\n", emoji, i, toupper(r$model)))
  cat(sprintf("    ROC-AUC: %.4f | F1: %.4f\n\n", r$roc_auc, r$f1))
}

# Ranking chart
results_df %>%
  mutate(rank = row_number()) %>%
  ggplot(aes(x = reorder(toupper(model), -roc_auc), y = roc_auc, fill = model)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = sprintf("%.4f", roc_auc)), vjust = -0.5) +
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.1))) +
  labs(
    title = "Model Ranking by ROC-AUC",
    x = NULL, y = "ROC-AUC"
  ) +
  theme_minimal()
```

<br>

# Recommendations

## When to Use Each Strategy

| Scenario | Recommendation | Reason |
|----------|---------------|--------|
| **Real-time Processing** | XGBoost (Primary) | Lowest latency (51Î¼s) |
| **High-Value Transactions** | Ensemble | Maximum accuracy |
| **Resource Constrained** | Single Model | Less memory/compute |
| **Batch Processing** | Ensemble | Latency less critical |
| **Production Default** | XGBoost | Best balance of speed and accuracy |

## Configuration for Rust Pipeline

```{r rust-config, echo=TRUE, eval=FALSE}
# config/config.toml

# For fast, single-model inference (recommended)
[models]
strategy = "primary"
primary_model = "xgboost"

[models.thresholds]
xgboost = 0.61
catboost = 0.57
lightgbm = 0.60
random_forest = 0.33
ensemble = 0.56

[detection]
threshold = 0.61  # XGBoost optimal
```

## Save Results

```{r save-results}
# Save model comparison
results_df %>%
  select(model, accuracy, precision, recall, f1, roc_auc, threshold_used, optimal_threshold) %>%
  write_csv("results/model_comparison.csv")

# Save thresholds
results_df %>%
  select(model, optimal_threshold) %>%
  write_csv("results/optimal_thresholds.csv")

cat("âœ“ Results saved to results/\n")
```

<br>

# Conclusion

```{r conclusion}
best <- results_df %>% filter(model != "ensemble") %>% slice(1)
ens <- results_df %>% filter(model == "ensemble")

cat("=== Analysis Complete ===\n\n")
cat(sprintf("Best Single Model: %s (ROC-AUC: %.4f)\n", toupper(best$model), best$roc_auc))
cat(sprintf("Ensemble Performance: ROC-AUC: %.4f\n", ens$roc_auc))
cat(sprintf("Ensemble Improvement: %+.2f%%\n", (ens$roc_auc - best$roc_auc) / best$roc_auc * 100))
cat("\nRecommendations:\n")
cat(sprintf("  1. Use %s as primary model with threshold %.2f\n", toupper(best$model), best$optimal_threshold))
cat("  2. Use ensemble for high-stakes decisions\n")
cat("  3. Monitor precision-recall based on business costs\n")
```

<br>

*Analysis completed on `r format(Sys.time(), "%B %d, %Y at %H:%M")`*
